import json
import os
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
INPUT_FILE = 'vk_dataset.json'
DB_DIRECTORY = 'vk_vector_db'  # –ü–∞–ø–∫–∞, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –±–∞–∑–∞

# –ú–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞. 
# MiniLM - –±—ã—Å—Ç—Ä–∞—è –∏ –ª–µ–≥–∫–∞—è. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —Å—É–ø–µ—Ä-–∫–∞—á–µ—Å—Ç–≤–æ, –≤–æ–∑—å–º–∏—Ç–µ 'intfloat/multilingual-e5-large'
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

def load_and_process_data(filepath):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç JSON –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –≤ Documents LangChain"""
    if not os.path.exists(filepath):
        print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞—Ä—Å–µ—Ä.")
        return []

    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    documents = []
    for item in data:
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã, —á—Ç–æ–±—ã RAG –º–æ–≥ —Å–∫–∞–∑–∞—Ç—å: "–Ø –≤–∑—è–ª —ç—Ç–æ –∏–∑ –ø–æ—Å—Ç–∞ –æ—Ç —Ç–∞–∫–æ–≥–æ-—Ç–æ —á–∏—Å–ª–∞ (—Å—Å—ã–ª–∫–∞)"
        metadata = {
            "source_id": item['id'],
            "date": item['date'],
            "likes": item['likes'],
            "url": item['url']
        }
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç. page_content - —ç—Ç–æ —Ç–æ, –ø–æ —á–µ–º—É –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å —Å–º—ã—Å–ª.
        doc = Document(page_content=item['text'], metadata=metadata)
        documents.append(doc)
    
    return documents

def create_vector_db(documents):
    if not documents:
        return

    print(f"üîÑ –†–∞–∑–±–∏–≤–∞–µ–º {len(documents)} –ø–æ—Å—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    
    # 1. –ß–∞–Ω–∫–∏–Ω–≥ (Text Splitting)
    # chunk_size=1000: —Ä–∞–∑–º–µ—Ä –∫—É—Å–æ—á–∫–∞ —Ç–µ–∫—Å—Ç–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
    # chunk_overlap=200: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–º—ã—Å–ª –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —Ä–∞–∑—Ä—ã–≤–∞
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"‚úÇÔ∏è –ü–æ–ª—É—á–∏–ª–æ—Å—å {len(splits)} —á–∞–Ω–∫–æ–≤ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤).")

    print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({MODEL_NAME})...")
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU, –µ—Å–ª–∏ –Ω–µ—Ç CUDA. –≠—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ CPU.
    embedding_model = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={'device': 'cpu'} 
    )

    print("üíæ –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
    # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –Ω–∞ –¥–∏—Å–∫
    vector_db = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        persist_directory=DB_DIRECTORY
    )
    
    print(f"‚úÖ –ë–∞–∑–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É '{DB_DIRECTORY}'")
    return vector_db

def test_search(query):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –±–∞–∑–µ"""
    print(f"\nüîé –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    embedding_model = HuggingFaceEmbeddings(model_name=MODEL_NAME)
    db = Chroma(persist_directory=DB_DIRECTORY, embedding_function=embedding_model)
    
    # –ò—â–µ–º 3 —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
    results = db.similarity_search(query, k=3)
    
    for i, res in enumerate(results):
        print(f"\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1} (–õ–∞–π–∫–æ–≤: {res.metadata['likes']}) ---")
        print(f"–¢–µ–∫—Å—Ç: {res.page_content[:200]}...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
        print(f"–°—Å—ã–ª–∫–∞: {res.metadata['url']}")

if __name__ == "__main__":
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º
    docs = load_and_process_data(INPUT_FILE)
    
    # 2. –°–æ–∑–¥–∞–µ–º –±–∞–∑—É (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–¥–∏–Ω —Ä–∞–∑, –ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å)
    create_vector_db(docs)
    
    # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    test_search("–ö–∞–∫ –≤–æ–π—Ç–∏ –≤ IT?")
